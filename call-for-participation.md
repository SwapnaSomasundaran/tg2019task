TextGraphs-13 Shared Task on Explanation Regeneration
=====================================================

Overview
--------

Developing methods of automated inference that are able to provide users with compelling human-readable justifications for why the
answer to a question is correct is critical for domains such as science and medicine, where user trust and detecting costly errors are limiting factors to adoption. One of the central barriers to training question answering models on explainable inference tasks is the lack of gold explanations to serve as training data.

This shared task focuses on TODO: describe the shared task at a high level. 

Important Dates
---------------

1. __xx-xx-xxxx__: Example (trial) data release
2. __xx-xx-xxxx__: Training data release
3. __xx-xx-xxxx__: Test data release. Evaluation start
4. __xx-xx-xxxx__: Evaluation end
5. __xx-xx-xxxx__: System description paper deadline
6. __xx-xx-xxxx__: Deadline for reviews of system description papers
7. __xx-xx-xxxx__: Author notifications
8. __xx-xx-xxxx__: Camera-ready description paper deadline
9. __11-03-2019/11-04-2019__: TextGraphs-13 workshop


Data
----

The data used in this shared task comes from the WorldTree corpus (Jansen et al., 2018). The data includes TODO: discuss data organization. Include 1 example question, correct/incorrect answers, and explanation for the correct answer. 

Participating systems will be evaluated using TODO: discuss evaluation measure. 

The shared task data distribution includes a baseline that TODO: discuss how the baseline works. The performance of this baseline on the development partition is TODO: summarize the baseline performance. 


Terms and Conditions
--------------------

By submitting results to this competition, you consent to the public release of your scores at the TextGraph-13 workshop and in the associated proceedings, at the task organizers' discretion. Scores may include, but are not limited to, automatic and manual quantitative judgements, qualitative judgements, and such other metrics as the task organizers see fit. You accept that the ultimate decision of metric choice and score value is that of the task organizers.

You further agree that the task organizers are under no obligation to release scores and that scores may be withheld if it is the task organizers' judgement that the submission was incomplete, erroneous, deceptive, or violated the letter or spirit of the competition's rules. Inclusion of a submission's scores is not an endorsement of a team or individual's submission, system, or science.

You further agree that your system may be named according to the team name provided at the time of submission, or to a suitable shorthand as determined by the task organizers.

You agree not to redistribute the test data except in the manner prescribed by its licence.

References
----------

```
@InProceedings{jansen2018worldtree,
    author = {Peter Jansen and Elizabeth Wainwright and Steven Marmorstein and Clayton T. Morrison},
    title = {WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference},
    booktitle = {Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC)},
    year = {2018},
    url = {http://cognitiveai.org/wp-content/uploads/2018/02/jansen_et_al_lrec2018_worldtree_computable_explanation_corpus_8pg_cameraready.pdf},
    url_code = {http://cognitiveai.org/explanationbank/}
}
```



